{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification algorithm that operates without any training, by simply comparing the new data point to the existing data points and assigning the label of the closest data point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by representing the training set of $N$ examples as $$ \\{(\\mathbf{x}^{(1)}, t^{(1)}), \\ldots, (\\mathbf{x}^{(N)}, t^{(N)})\\} $$ where $\\mathbf{x}^{(i)}$ is the $i^{th}$ data point as a vector (one column for each feature) and $t^{(i)}$ is the label of the $i^{th}$ data point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then observe a new input vector $\\mathbf{x}$ and we want to predict its label $t$. We decide to look at our dataset and choose the label by selecting the exact same label of the closest data point to $\\mathbf{x}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of closest may be Euclidean distance \n",
    "$$ ||\\mathbf{x}^{(a)} - \\mathbf{x}^{(b)}||_2 = \\sqrt{\\sum_{j=1}^d (x^{(a)}_j - x^{(b)}_j)^2} $$\n",
    "but other distance metrics will work as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "1. Find example $(\\mathbf{x}^{(*)}, t^{(*)})$ in the training set $\\mathcal{S}$ closest to $\\mathbf{x}$. That is:\n",
    "\t$$ \\mathbf{x}^{(*)} = \\argmin_{\\mathbf{x}^{(i)} \\in \\mathcal{S} } \\texttt{distance}(\\mathbf{x}, \\mathbf{x}^{(i)}) $$\n",
    "   $\\mathbf{x}^{(*)}$ is the argument that minimizes the distance function. Note that there may be multiple values that minimize the distance, in which case we can choose one arbitrarily.\n",
    "2. Return $y = t^{(*)}$ as the predicted label for $\\mathbf{x}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you use Euclidean distance, you can skip the square root operation since it does not change the ordering of the distances (it is a monotonic function)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is still too sensitive to noisy or mislabeled data. If an outlier point is a different class from all its neighbors, and those neighbors are all the same class, we expect points with features similar to those points to also be of the neighbor's class, but if they're too close to the outlier, they will be misclassified.\n",
    "\n",
    "We can get around this by using a majority vote of the $k$ nearest neighbors instead of just the closest one. This is called the $k$-nearest neighbors algorithm.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm(kNN)\n",
    "1. Find $k$ examples $(\\mathbf{x}^{(i)}, t^{(i)})$ in the training set $\\mathcal{S}$ closest to $\\mathbf{x}$ and select their labels as $\\mathcal{Y} = \\{t^{(1)}, \\ldots, t^{(k)}\\}$.\n",
    "2. Choose $y$ as the plurality label among the $k$ neighbors.\n",
    "\t$$ y^* = \\max_{t^{(z)} \\in \\texttt{all\\_labels} } \\sum_{i=1}^k \\mathbb{I}(t^{(i)} = t^{(z)}) $$\n",
    "Try each label as $t^{(z)}$ and select the one that makes the sum over $\\mathcal{Y}$ of the identity functions $\\mathbb{I}$, which outputs 1 when the statement is true and 0 otherwise, the largest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$\n",
    " - Small $k$ makes the algorithm better at capturing fine details of the data, but more sensitive to noise. More likely to overfit.\n",
    " - Large $k$ makes the algorithm better at capturing the general trend of the data, but less sensitive to noise. More likely to underfit.\n",
    "The best value of $k$ depends on the number of data points $n$. It's best if $k$ can grow while $\\frac{k}{n} \\to 0$. A good rule of thumb is to choose $k$ such that $k = \\sqrt{n}$.\n",
    "\n",
    "In practice $k$ is a hyperparameter that can be experimented with using validation sets to find the best value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings\n",
    " - The algorithm works best in low dimensions (when the number of features per data point is small). In high dimensions, the curse of dimensionality means most points are far away and approximately the same distance.\n",
    " - We need many data points to cover the entire space within a certain $\\epsilon$ distance of a point. \n",
    " - The ranges of the features may unintentionally clump the data points together in some dimensions and spread them out in others. This can be fixed by normalizing the data. Make each dimension have zero mean and unit variance which can be done by computing the mean $\\mu_j$ and standard deviation $\\sigma_j$ of each feature $j$ and then setting the normalized value of each data point as: $$x'_j = \\frac{x_j - \\mu_j}{\\sigma_j}$$\n",
    " - Slow, since the algorithm needs to calculate $D$ dimensional distances for $N$ points so it takes $O(DN)$ time. This must then be sorted in $O(N \\log N)$ time. This is done for each query, though there are obviously many optimizations to be had here.\n",
    "> In high dimensions, the number of points needed grows exponentially with the number of dimensions. The volume of a single ball of radius $\\epsilon$ around each point grows like $O(\\epsilon^d)$. \n",
    "> The total volume of the unit hypercube $[0, 1]^d$ is 1.\n",
    "> Therefore we need on the order of $O(\\left(\\frac{1}{\\epsilon}\\right)^d)$ points to cover the volume.\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"StatsBase\")\n",
    "using DataFrames # for dataframes\n",
    "using LinearAlgebra # for norm\n",
    "using StatsBase # for mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Car\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we do a simplified example where we have a 1 feature (wheels) dataset with 10 data points and we need to select the category of a new point\n",
    "data = Dict(\n",
    "\t:category => [\"Car\", \"Truck\", \"Motorcycle\", \"Car\", \"Car\", \"Truck\", \"Motorcycle\", \"Motorcycle\", \"Truck\", \"Car\"],\n",
    "\t:wheels => [4, 8, 2, 4, 4, 18, 2, 3, 6, 4]\n",
    ")\n",
    "dataset = DataFrame(data)\n",
    "# get a new point\n",
    "new = 4\n",
    "X = dataset[!, \"wheels\"] # get the wheels column\n",
    "# norm by default is the euclidean distance, but its second argument controls it\n",
    "# get the distance for each point with the new point\n",
    "distances = [norm(X[i] - new) for i in 1:length(X)]\n",
    "indices = sortperm(distances)[1:3] # get the indices of the 3 closest points\n",
    "Y = dataset[!, \"category\"]\n",
    "nearest = Y[indices]\n",
    "# get the most frequent category\n",
    "mode(nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "knn (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# given X with n rows with each row corresponding to a data point and a vector Y with the category of each corresponding data point \n",
    "# return the category of the input point using the k nearest neighbors algorithm given the value of k\n",
    "function knn(X, Y, k, input)\n",
    "\t# distances is a vector of vectors, where each element is a column vector that's a row of X\n",
    "\t# that's what X[i, :] does, it gets the ith row of X as a column vector\n",
    "\t# if we had d features, each example would become a dx1 matrix (column vector) and x would be a dxn matrix\n",
    "\t# calculate the l_2 distance between the input point and each point in the dataset\n",
    "\tdistances = [norm(X[i, :] - input) for i in 1:size(X, 1)]\n",
    "\t# sort the distances and get the indices of the k nearest points\n",
    "\tindices = sortperm(distances)[1:k]\n",
    "\t# get the categories of the k nearest points\n",
    "\tnearest_categories = Y[indices]\n",
    "\t# return the most common category\n",
    "\treturn mode(nearest_categories)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's try another dataset with 2 features\n",
    "houses = Dict(\n",
    "\t:price => [300, 400, 450, 620, 700, 670, 990, 800, 1100, 1000],\n",
    "\t:area => [150, 120, 150, 180, 400, 220, 220, 280, 300, 400],\n",
    "\t:rooms => [2, 2, 3, 3, 4, 4, 5, 5, 6, 6]\n",
    ")\n",
    "# here we'll try to predict the number of rooms given the price and area\n",
    "houses = DataFrame(houses)\n",
    "X = [houses[!, \"price\"] houses[!, \"area\"]]\n",
    "Y = houses[!, \"rooms\"]\n",
    "input = [500, 200] # first col is price, second is area, must match the same shape as X\n",
    "knn(X, Y, 3, input) # => 3 rooms\n",
    "# here we can consider normalizing the axes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
